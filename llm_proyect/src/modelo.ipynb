{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "from datasets import Dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar dispositivo como variable global\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria GPU asignada: 0.00 MB\n",
      "Memoria GPU reservada: 0.00 MB\n",
      "Memoria libre: 4095.56 MB\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_memory():\n",
    "    allocated_memory = torch.cuda.memory_allocated()\n",
    "    cached_memory = torch.cuda.memory_reserved()\n",
    "    print(f\"Memoria GPU asignada: {allocated_memory / 1024 ** 2:.2f} MB\")\n",
    "    print(f\"Memoria GPU reservada: {cached_memory / 1024 ** 2:.2f} MB\")\n",
    "    print(f\"Memoria libre: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 2 - allocated_memory / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "check_gpu_memory()  # Verifica antes de entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\backend.py:277: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Sesi√≥n de TensorFlow limpiada.\n",
      "Verificando el estado de la memoria de la GPU despu√©s de limpiar...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clear_gpu_memory():\n",
    "    # Comando para restablecer la GPU usando nvidia-smi\n",
    "    os.system('nvidia-smi --gpu-reset -i 0')\n",
    "\n",
    "    # Limpiar la cach√© de memoria de CUDA\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Tambi√©n puedes intentar vaciar la cach√© de la memoria de TensorFlow, si lo est√°s usando\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.keras.backend.clear_session()\n",
    "        print(\"Sesi√≥n de TensorFlow limpiada.\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    # Verificaci√≥n de memoria\n",
    "    print(\"Verificando el estado de la memoria de la GPU despu√©s de limpiar...\")\n",
    "    time.sleep(2)  # Un peque√±o retraso para asegurar que los cambios se apliquen\n",
    "    os.system('nvidia-smi')\n",
    "\n",
    "# Llamada a la funci√≥n para limpiar la memoria de la GPU\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()  # Carga variables del archivo .env\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Asignar el token de padding al token de \"fin de oraci√≥n\" (eos_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.model_max_length = 1024  # Asegura que el tokenizador usa el l√≠mite correcto\n",
    "model.to(device)  # Mover el modelo a la GPU si est√° disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                texto\n",
      "39   Lis√≠maco Hola, Nicias y Laques, ¬øhab√©is visto...\n",
      "53  El , muy superior a los di√°logos inmediatament...\n",
      "32   Criton S√≥crates, ¬øqui√©n era aquel hombre con ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Carga de datos\n",
    "\"\"\"\n",
    "# Rutas de los archivos procesados\n",
    "ruta_argumentos = r\"C:\\Users\\Lucas\\Desktop\\Socrates\\llm_proyect\\data\\limpios\\argumentos_procesados.csv\"\n",
    "ruta_dialogos = r\"C:\\Users\\Lucas\\Desktop\\Socrates\\llm_proyect\\data\\limpios\\dialogos_procesados.csv\"\n",
    "ruta_escritos = r\"C:\\Users\\Lucas\\Desktop\\Socrates\\llm_proyect\\data\\limpios\\escritos_procesados.csv\"\n",
    "\n",
    "# Cargar los datos\n",
    "df_argumentos = pd.read_csv(ruta_argumentos)\n",
    "df_dialogos = pd.read_csv(ruta_dialogos)\n",
    "df_escritos = pd.read_csv(ruta_escritos)\n",
    "\n",
    "# Combinar textos en un solo DataFrame\n",
    "df = pd.concat([\n",
    "    df_argumentos[['argumento']].rename(columns={'argumento': 'texto'}),\n",
    "    df_dialogos[['dialogo']].rename(columns={'dialogo': 'texto'}),\n",
    "    df_escritos[['contenido']].rename(columns={'contenido': 'texto'})\n",
    "], ignore_index=True)\n",
    "\n",
    "# Eliminar textos vac√≠os o nulos\n",
    "df.dropna(subset=['texto'], inplace=True)\n",
    "\n",
    "# Mostrar un ejemplo\n",
    "print(df.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:01<00:00, 62.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tokenizaci√≥n y preparaci√≥n del Dataser\n",
    "\"\"\"\n",
    "# Cargar el modelo y el tokenizador GPT-2\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Configurar el token de padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.model_max_length = 1024\n",
    "model.to(device)\n",
    "\n",
    "# Tokenizaci√≥n\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['texto'], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "# Convertir a Dataset de Hugging Face\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Dividir en entrenamiento y validaci√≥n\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuracion de Entrenamiento\n",
    "\"\"\"\n",
    "\n",
    "# Configurar los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,               # N√∫mero de √©pocas de entrenamiento\n",
    "    per_device_train_batch_size=4,    # Tama√±o de lote para entrenamiento\n",
    "    per_device_eval_batch_size=4,     # Tama√±o de lote para validaci√≥n\n",
    "    evaluation_strategy=\"epoch\",      # Evaluar al final de cada √©poca\n",
    "    save_strategy=\"epoch\",            # Guardar modelo al final de cada √©poca\n",
    "    warmup_steps=500,                 # Calentamiento para el scheduler de LR\n",
    "    weight_decay=0.01,                # Regularizaci√≥n L2\n",
    "    logging_dir=\"./logs\",             # Directorio para logs\n",
    "    logging_steps=50,                 # Frecuencia de logging\n",
    "    fp16=True if torch.cuda.is_available() else False  # Habilitar FP16 si hay GPU\n",
    ")\n",
    "\n",
    "# Inicializar el Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Entrenamiento del Modelo\n",
    "\"\"\"\n",
    "# Iniciar el entrenamiento\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta para guardar el modelo entrenado\n",
    "save_path = r\"C:\\Users\\Lucas\\Desktop\\Socrates\\llm_proyect\\data\\models\"\n",
    "\n",
    "# Guardar el modelo y el tokenizador\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Modelo y tokenizador guardados en: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluaci√≥n rapida del modelo\n",
    "\"\"\"\n",
    "# Generar texto de prueba\n",
    "input_text = \"S√≥crates dijo:\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generar texto con el modelo entrenado\n",
    "output = model.generate(input_ids, max_length=150, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
    "\n",
    "# Decodificar y mostrar el resultado\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
