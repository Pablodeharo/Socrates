{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "from datasets import Dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar dispositivo como variable global\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria GPU asignada: 0.00 MB\n",
      "Memoria GPU reservada: 0.00 MB\n",
      "Memoria libre: 4095.56 MB\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_memory():\n",
    "    allocated_memory = torch.cuda.memory_allocated()\n",
    "    cached_memory = torch.cuda.memory_reserved()\n",
    "    print(f\"Memoria GPU asignada: {allocated_memory / 1024 ** 2:.2f} MB\")\n",
    "    print(f\"Memoria GPU reservada: {cached_memory / 1024 ** 2:.2f} MB\")\n",
    "    print(f\"Memoria libre: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 2 - allocated_memory / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "check_gpu_memory()  # Verifica antes de entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\backend.py:277: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Sesión de TensorFlow limpiada.\n",
      "Verificando el estado de la memoria de la GPU después de limpiar...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clear_gpu_memory():\n",
    "    # Comando para restablecer la GPU usando nvidia-smi\n",
    "    os.system('nvidia-smi --gpu-reset -i 0')\n",
    "\n",
    "    # Limpiar la caché de memoria de CUDA\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # También puedes intentar vaciar la caché de la memoria de TensorFlow, si lo estás usando\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.keras.backend.clear_session()\n",
    "        print(\"Sesión de TensorFlow limpiada.\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    # Verificación de memoria\n",
    "    print(\"Verificando el estado de la memoria de la GPU después de limpiar...\")\n",
    "    time.sleep(2)  # Un pequeño retraso para asegurar que los cambios se apliquen\n",
    "    os.system('nvidia-smi')\n",
    "\n",
    "# Llamada a la función para limpiar la memoria de la GPU\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()  # Carga variables del archivo .env\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Asignar el token de padding al token de \"fin de oración\" (eos_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.model_max_length = 1024  # Asegura que el tokenizador usa el límite correcto\n",
    "model.to(device)  # Mover el modelo a la GPU si está disponible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
